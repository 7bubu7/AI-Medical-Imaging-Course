üìò Lesson 5 - Vision Transformer (ViT) & Binary Medical Classification

In this lesson, we explored how transformer-based architectures, specifically Vision Transformers (ViT), can be used in medical imaging tasks. We applied ViT and CNNs to two binary classification problems: **brain MRI tumor classification** and **pneumonia detection**.

---

 üß† What I Learned

- Transformer architecture and self-attention mechanisms
- Vision Transformer (ViT) theory and implementation in PyTorch
- Comparison between CNN and ViT in binary classification tasks
- Evaluation metrics: Accuracy, Precision, Recall, F1-score, Confusion Matrix, ROC-AUC
- Handling imbalanced data in binary classification

---

üîß Tools Used

- Google Colab  
- PyTorch  
- Torchvision  
- ViT from `torchvision.models`  
- Matplotlib  

---

 üìÅ Files

- `Vision Transformer.ipynb`: Demonstration of ViT on image data
- `Brain MRI Binary Classification PyTorch ViT.ipynb`: Tumor classification using ViT on MRI dataset
- `[HW]PyTorch pneumonia.ipynb`: Homework - Pneumonia detection using CNN or ViT

---

 üèÅ Outcome

This lesson expands the toolkit for AI-based medical imaging by introducing transformer-based models. Through hands-on binary classification tasks, I explored ViT's potential and compared it with traditional CNN approaches in clinical settings.
